{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169a28be",
   "metadata": {},
   "source": [
    "# Hava KirliliÄŸi / Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa610f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneyi kuralÄ±m\n",
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Hangi yÄ±llarÄ± indirmek istiyoruz?\n",
    "HEDEFLENEN_YILLAR = [2020,2021,2022,2023,2024,2025]\n",
    "\n",
    "# Hangi veri setlerini indireceÄŸiz? (Daha Ã¶nce test ettiklerimiz)\n",
    "# ('NASA Veri Seti KÄ±sa AdÄ±', 'Versiyon NumarasÄ±')\n",
    "VERI_SETLERI = [\n",
    "    ('OMNO2d', '003'),                          # OMI-Aura NOâ‚‚ verisi (.he5)\n",
    "]\n",
    "\n",
    "# Verilerin indirileceÄŸi ana klasÃ¶r\n",
    "INDIRME_KLASORU = \"NASA_Verileri_2\"\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriÅŸ yapÄ±lÄ±yor...\")\n",
    "try:\n",
    "    earthaccess.login()\n",
    "    print(\"âœ… GiriÅŸ baÅŸarÄ±lÄ±.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GiriÅŸ baÅŸarÄ±sÄ±z. LÃ¼tfen yukarÄ±daki .netrc oluÅŸturma adÄ±mÄ±nÄ± kontrol edin. Hata: {e}\")\n",
    "    # GiriÅŸ baÅŸarÄ±sÄ±z olursa kodu burada durdur.\n",
    "    exit()\n",
    "\n",
    "# Ana indirme klasÃ¶rÃ¼nÃ¼ oluÅŸtur\n",
    "Path(INDIRME_KLASORU).mkdir(exist_ok=True)\n",
    "\n",
    "# BugÃ¼nÃ¼n tarihini al (2025 yÄ±lÄ± iÃ§in bitiÅŸ tarihi olarak kullanÄ±lacak)\n",
    "bugun = datetime.now()\n",
    "\n",
    "for short_name, version in VERI_SETLERI:\n",
    "    for yil in HEDEFLENEN_YILLAR:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Ä°ÅŸlem baÅŸlÄ±yor: Veri Seti='{short_name}', YÄ±l='{yil}'\")\n",
    "        \n",
    "        # Ä°ndirme iÃ§in alt klasÃ¶r oluÅŸtur (Ã¶rn: NASA_Verileri/OMNO2d/2020)\n",
    "        hedef_klasor = Path(INDIRME_KLASORU) / short_name / str(yil)\n",
    "        hedef_klasor.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Dosyalar '{hedef_klasor}' klasÃ¶rÃ¼ne indirilecek.\")\n",
    "\n",
    "        # Zaman aralÄ±ÄŸÄ±nÄ± belirle\n",
    "        baslangic_tarihi = f\"{yil}-01-01\"\n",
    "        if yil == bugun.year:\n",
    "            # EÄŸer yÄ±l 2025 ise, bitiÅŸ tarihi bugÃ¼n olsun\n",
    "            bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            # DiÄŸer yÄ±llar iÃ§in tÃ¼m yÄ±lÄ± kapsa\n",
    "            bitis_tarihi = f\"{yil}-12-31\"\n",
    "            \n",
    "        print(f\"Zaman AralÄ±ÄŸÄ±: {baslangic_tarihi} -> {bitis_tarihi}\")\n",
    "\n",
    "        # NASA sunucularÄ±nda veri arama\n",
    "        try:\n",
    "            sonuclar = earthaccess.search_data(\n",
    "                short_name=short_name,\n",
    "                version=version,\n",
    "                temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "            )\n",
    "            \n",
    "            if not sonuclar:\n",
    "                print(\"âš ï¸ Bu tarih aralÄ±ÄŸÄ± iÃ§in hiÃ§ dosya bulunamadÄ±.\")\n",
    "                continue # Bir sonraki yÄ±la veya veri setine geÃ§\n",
    "\n",
    "            print(f\"âœ… {len(sonuclar)} adet dosya bulundu. Ä°ndirme iÅŸlemi baÅŸlÄ±yor...\")\n",
    "            \n",
    "            # Bulunan dosyalarÄ± indir\n",
    "            earthaccess.download(sonuclar, local_path=str(hedef_klasor))\n",
    "            print(f\"âœ¨ '{short_name}' iÃ§in {yil} yÄ±lÄ± verileri baÅŸarÄ±yla indirildi.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ '{short_name}' iÃ§in {yil} yÄ±lÄ±nda veri aranÄ±rken/indirilirken bir hata oluÅŸtu: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TÃ¼m indirme iÅŸlemleri tamamlandÄ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97027e40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rioxarray\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np # KoordinatlarÄ± oluÅŸturmak iÃ§in eklendi\n",
    "\n",
    "# --- AYARLAR BÃ–LÃœMÃœ ---\n",
    "VERI_KLASORU = \"NASA_Verileri/OMNO2d/2020\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "CIKTI_CSV_DOSYASI = \"sehir_bazli_no2_skorlari_2020.csv\"\n",
    "\n",
    "# Verinin bulunduÄŸu tek grup yolu\n",
    "HDF5_DATA_GROUP = 'HDFEOS/GRIDS/ColumnAmountNO2/Data Fields'\n",
    "\n",
    "DEGISKEN_ADLARI_LISTESI = [\n",
    "    'ColumnAmountNO2TropCloudScreened',\n",
    "    'ColumnAmountNO2Trop'\n",
    "]\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "def final_skorlari_hesapla():\n",
    "    print(f\"Yerel '{GEOJSON_DOSYASI}' dosyasÄ±ndan il sÄ±nÄ±rlarÄ± okunuyor...\")\n",
    "    try:\n",
    "        iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "        iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "        print(f\"âœ… {len(iller_gdf)} adet il baÅŸarÄ±yla yÃ¼klendi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ HATA: '{GEOJSON_DOSYASI}' dosyasÄ± okunamadÄ±. -> {e}\")\n",
    "        return\n",
    "\n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    he5_dosyalari = sorted(list(veri_yolu.glob('*.he5')))\n",
    "    if not he5_dosyalari:\n",
    "        print(f\"âŒ HATA: '{veri_yolu}' klasÃ¶rÃ¼nde dosya bulunamadÄ±.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Ä°ÅŸlenmek Ã¼zere {len(he5_dosyalari)} adet dosya bulundu.\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(he5_dosyalari, desc=\"GÃ¼nlÃ¼k Veriler Ä°ÅŸleniyor\"):\n",
    "        try:\n",
    "            # === YENÄ° VE KESÄ°N YÃ–NTEM ===\n",
    "            # 1. Sadece veri iÃ§eren grubu aÃ§\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_DATA_GROUP)\n",
    "            \n",
    "            # 2. AkÄ±llÄ± deÄŸiÅŸken seÃ§imi\n",
    "            kullanilacak_degisken = next((var for var in DEGISKEN_ADLARI_LISTESI if var in ds_data.data_vars), None)\n",
    "            if not kullanilacak_degisken:\n",
    "                continue\n",
    "\n",
    "            rds = ds_data[kullanilacak_degisken]\n",
    "\n",
    "            # 3. KoordinatlarÄ± sÄ±fÄ±rdan oluÅŸtur ve veriye ata\n",
    "            # BoyutlarÄ± yeniden adlandÄ±r\n",
    "            rds = rds.rename({rds.dims[0]: 'y', rds.dims[1]: 'x'})\n",
    "            \n",
    "            # Global grid iÃ§in koordinat dizilerini oluÅŸtur\n",
    "            # 1440 piksel -180'den +180'e (boylam), 720 piksel +90'dan -90'a (enlem)\n",
    "            lon_coords = np.linspace(-180 + (0.25/2), 180 - (0.25/2), 1440)\n",
    "            lat_coords = np.linspace(90 - (0.25/2), -90 + (0.25/2), 720)\n",
    "            \n",
    "            # OluÅŸturulan koordinatlarÄ± veriye ata\n",
    "            rds = rds.assign_coords(x=lon_coords, y=lat_coords)\n",
    "            \n",
    "            # CoÄŸrafi referans sistemini belirt\n",
    "            rds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "            # === ESKÄ° KOD DEVAM EDÄ°YOR ===\n",
    "            tarih_str_match = re.search(r'_(\\d{4}m\\d{4})_', str(dosya_path))\n",
    "            if not tarih_str_match: continue\n",
    "            tarih = pd.to_datetime(tarih_str_match.group(1).replace('m', '-'), format='%Y-%m%d').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(rds.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = rds.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_skor = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_skor) and ortalama_skor > 0:\n",
    "                        tum_sonuclar.append({\n",
    "                            \"sehir\": sehir_adi,\n",
    "                            \"tarih\": tarih,\n",
    "                            \"no2_skoru\": ortalama_skor\n",
    "                        })\n",
    "                except Exception:\n",
    "                    # Bu ÅŸehir iÃ§in veri bulunamazsa (Ã¶rn. okyanusa denk gelirse) atla\n",
    "                    continue\n",
    "        except Exception:\n",
    "            # print(f\"\\nâš ï¸ HATA: '{dosya_path.name}' iÅŸlenirken genel bir sorun oluÅŸtu. AtlanÄ±yor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"âŒ ÃœzgÃ¼nÃ¼m, yine hiÃ§bir dosya baÅŸarÄ±yla iÅŸlenemedi. Veri yapÄ±sÄ±nda beklenmedik bir durum olabilir.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTÃ¼m veriler iÅŸlendi. CSV dosyasÄ± oluÅŸturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df['no2_skoru'] = sonuc_df['no2_skoru'] * 1e15\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"âœ¨ BaÅŸarÄ±lÄ±! SonuÃ§lar '{CIKTI_CSV_DOSYASI}' dosyasÄ±na kaydedildi.\")\n",
    "    print(\"\\nCSV DosyasÄ±nÄ±n ilk 5 satÄ±rÄ±:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_skorlari_hesapla()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03f464",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# SÄ±caklÄ±k GÃ¼ndÃ¼z / Temperature Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31aa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'VNP21A1D'\n",
    "VERSIYON = '002'\n",
    "YIL = 2021 # <-- ArtÄ±k buraya yazdÄ±ÄŸÄ±n yÄ±l ne ise, sadece o indirilecek.\n",
    "\n",
    "# TÃ¼rkiye'yi iÃ§ine alan coÄŸrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceÄŸi klasÃ¶r\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriÅŸ yapÄ±lÄ±yor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"âœ… GiriÅŸ baÅŸarÄ±lÄ±.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GiriÅŸ baÅŸarÄ±sÄ±z. LÃ¼tfen .netrc dosyasÄ±nÄ± kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirilecek.\\n\")\n",
    "\n",
    "# === DÃœZELTME BURADA: BitiÅŸ tarihini YIL'a gÃ¶re doÄŸru ayarlayan mantÄ±k ===\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    # EÄŸer geÃ§miÅŸ bir yÄ±lÄ± arÄ±yorsak, bitiÅŸ tarihi o yÄ±lÄ±n sonu (31 AralÄ±k) olmalÄ±.\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    # EÄŸer iÃ§inde bulunduÄŸumuz yÄ±lÄ± (2025) arÄ±yorsak, bitiÅŸ tarihi bugÃ¼n olsun.\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "# === DÃœZELTME SONU ===\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, YÄ±l: {YIL}\")\n",
    "print(f\"Zaman AralÄ±ÄŸÄ±: {baslangic_tarihi} -> {bitis_tarihi}\") # Bu satÄ±r artÄ±k doÄŸru aralÄ±ÄŸÄ± gÃ¶sterecek\n",
    "print(f\"BÃ¶lge: TÃ¼rkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranÄ±yor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"âœ… {len(sonuclar)} adet dosya bulundu. Ä°ndirme iÅŸlemi baÅŸlÄ±yor...\")\n",
    "        print(\"â— Bu iÅŸlem uzun sÃ¼rebilir, lÃ¼tfen sabÄ±rlÄ± olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\nâœ¨ Ä°ndirme tamamlandÄ±! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš  Belirtilen tarih ve bÃ¶lge iÃ§in hiÃ§ dosya bulunamadÄ±.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Arama veya indirme sÄ±rasÄ±nda bir hata oluÅŸtu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecff740",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve iÅŸlenecek dosya sayÄ±sÄ±nÄ± belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 50 # HÄ±zlÄ± bir test iÃ§in sadece 5 dosya\n",
    "\n",
    "YIL = 2021\n",
    "VERI_KLASORU = f\"NASA_Verileri/VNP21A1D/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "# Ã‡Ä±ktÄ± dosya adÄ±nÄ± test moduna gÃ¶re ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_gunduz.csv\"\n",
    "\n",
    "# h5 dosyasÄ± iÃ§indeki doÄŸru grup ve deÄŸiÅŸken adlarÄ±\n",
    "HDF5_PARENT_GROUP = 'HDFEOS/GRIDS/VIIRS_Grid_Daily_1km_LST21'\n",
    "HDF5_DATA_SUBGROUP = 'Data Fields'\n",
    "HDF5_DEGISKENI = 'LST_1KM'\n",
    "\n",
    "# NASA VIIRS/MODIS verilerinin kullandÄ±ÄŸÄ± standart Sinusoidal Projeksiyon\n",
    "SINUSOIDAL_PROJ = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "def skorlari_hesapla_test():\n",
    "    iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "    iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "    \n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    h5_dosyalari = sorted(list(veri_yolu.glob('*.h5')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kÄ±salt\n",
    "    if TEST_MODU:\n",
    "        h5_dosyalari = h5_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"ğŸ TEST MODU AKTÄ°F: Sadece ilk {len(h5_dosyalari)} dosya iÅŸlenecek.\")\n",
    "    \n",
    "    print(f\"Ä°ÅŸlenmek Ã¼zere {len(h5_dosyalari)} adet sÄ±caklÄ±k dosyasÄ± seÃ§ildi.\")\n",
    "    print(\"Ä°ÅŸlem baÅŸlÄ±yor...\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(h5_dosyalari, desc=\"GÃ¼nlÃ¼k SÄ±caklÄ±k Verileri Ä°ÅŸleniyor\"):\n",
    "        try:\n",
    "            data_group_path = f\"{HDF5_PARENT_GROUP}/{HDF5_DATA_SUBGROUP}\"\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=data_group_path)\n",
    "            ds_coords = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_PARENT_GROUP)\n",
    "\n",
    "            lst_data = ds_data[HDF5_DEGISKENI]\n",
    "            \n",
    "            lst_data = lst_data.rename({lst_data.dims[0]: 'y', lst_data.dims[1]: 'x'})\n",
    "            lst_data = lst_data.assign_coords({\"y\": ds_coords['YDim'].values, \"x\": ds_coords['XDim'].values})\n",
    "            \n",
    "            if '_FillValue' in lst_data.attrs:\n",
    "                lst_data = lst_data.where(lst_data != lst_data.attrs['_FillValue'])\n",
    "            if 'scale_factor' in lst_data.attrs:\n",
    "                lst_data = lst_data * lst_data.attrs['scale_factor']\n",
    "            lst_celsius = lst_data - 273.15\n",
    "            \n",
    "            lst_celsius.rio.write_crs(SINUSOIDAL_PROJ, inplace=True)\n",
    "            \n",
    "            tarih_match = re.search(r'\\.A(\\d{7})\\.', str(dosya_path))\n",
    "            if not tarih_match: continue\n",
    "            tarih = pd.to_datetime(tarih_match.group(1), format='%Y%j').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(lst_celsius.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = lst_celsius.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_sicaklik = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_sicaklik):\n",
    "                        tum_sonuclar.append({\"sehir\": sehir_adi, \"tarih\": tarih, \"sicaklik_C\": ortalama_sicaklik})\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ HATA: '{dosya_path.name}' atlanÄ±yor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"\\nâŒ Test iÅŸlemi baÅŸarÄ±sÄ±z. HiÃ§bir veri iÅŸlenemedi.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTest verileri iÅŸlendi. CSV dosyasÄ± oluÅŸturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"âœ¨ Test BaÅŸarÄ±lÄ±! SonuÃ§lar '{CIKTI_CSV_DOSYASI}' dosyasÄ±na kaydedildi.\")\n",
    "    print(\"\\nCSV DosyasÄ±nÄ±n ilk 5 satÄ±rÄ±:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    skorlari_hesapla_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd04e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# SÄ±caklÄ±k Gece / Tempareture Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d29264",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'VNP21A1N'\n",
    "VERSIYON = '002'\n",
    "YIL = 2021 # <-- ArtÄ±k buraya yazdÄ±ÄŸÄ±n yÄ±l ne ise, sadece o indirilecek.\n",
    "\n",
    "# TÃ¼rkiye'yi iÃ§ine alan coÄŸrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceÄŸi klasÃ¶r\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriÅŸ yapÄ±lÄ±yor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"âœ… GiriÅŸ baÅŸarÄ±lÄ±.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GiriÅŸ baÅŸarÄ±sÄ±z. LÃ¼tfen .netrc dosyasÄ±nÄ± kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirilecek.\\n\")\n",
    "\n",
    "# === DÃœZELTME BURADA: BitiÅŸ tarihini YIL'a gÃ¶re doÄŸru ayarlayan mantÄ±k ===\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    # EÄŸer geÃ§miÅŸ bir yÄ±lÄ± arÄ±yorsak, bitiÅŸ tarihi o yÄ±lÄ±n sonu (31 AralÄ±k) olmalÄ±.\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    # EÄŸer iÃ§inde bulunduÄŸumuz yÄ±lÄ± (2025) arÄ±yorsak, bitiÅŸ tarihi bugÃ¼n olsun.\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "# === DÃœZELTME SONU ===\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, YÄ±l: {YIL}\")\n",
    "print(f\"Zaman AralÄ±ÄŸÄ±: {baslangic_tarihi} -> {bitis_tarihi}\") # Bu satÄ±r artÄ±k doÄŸru aralÄ±ÄŸÄ± gÃ¶sterecek\n",
    "print(f\"BÃ¶lge: TÃ¼rkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranÄ±yor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"âœ… {len(sonuclar)} adet dosya bulundu. Ä°ndirme iÅŸlemi baÅŸlÄ±yor...\")\n",
    "        print(\"â— Bu iÅŸlem uzun sÃ¼rebilir, lÃ¼tfen sabÄ±rlÄ± olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\nâœ¨ Ä°ndirme tamamlandÄ±! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš  Belirtilen tarih ve bÃ¶lge iÃ§in hiÃ§ dosya bulunamadÄ±.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Arama veya indirme sÄ±rasÄ±nda bir hata oluÅŸtu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cce1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve iÅŸlenecek dosya sayÄ±sÄ±nÄ± belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 50 # HÄ±zlÄ± bir test iÃ§in sadece 5 dosya\n",
    "\n",
    "YIL = 2021\n",
    "VERI_KLASORU = f\"NASA_Verileri/VNP21A1N/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "# Ã‡Ä±ktÄ± dosya adÄ±nÄ± test moduna gÃ¶re ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}.csv\"\n",
    "\n",
    "# h5 dosyasÄ± iÃ§indeki doÄŸru grup ve deÄŸiÅŸken adlarÄ±\n",
    "HDF5_PARENT_GROUP = 'HDFEOS/GRIDS/VIIRS_Grid_Daily_1km_LST21'\n",
    "HDF5_DATA_SUBGROUP = 'Data Fields'\n",
    "HDF5_DEGISKENI = 'LST_1KM'\n",
    "\n",
    "# NASA VIIRS/MODIS verilerinin kullandÄ±ÄŸÄ± standart Sinusoidal Projeksiyon\n",
    "SINUSOIDAL_PROJ = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "\n",
    "# --- KOD BAÅLANGICI ---\n",
    "\n",
    "def skorlari_hesapla_test():\n",
    "    iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "    iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "    \n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    h5_dosyalari = sorted(list(veri_yolu.glob('*.h5')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kÄ±salt\n",
    "    if TEST_MODU:\n",
    "        h5_dosyalari = h5_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"ğŸ TEST MODU AKTÄ°F: Sadece ilk {len(h5_dosyalari)} dosya iÅŸlenecek.\")\n",
    "    \n",
    "    print(f\"Ä°ÅŸlenmek Ã¼zere {len(h5_dosyalari)} adet sÄ±caklÄ±k dosyasÄ± seÃ§ildi.\")\n",
    "    print(\"Ä°ÅŸlem baÅŸlÄ±yor...\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(h5_dosyalari, desc=\"GÃ¼nlÃ¼k SÄ±caklÄ±k Verileri Ä°ÅŸleniyor\"):\n",
    "        try:\n",
    "            data_group_path = f\"{HDF5_PARENT_GROUP}/{HDF5_DATA_SUBGROUP}\"\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=data_group_path)\n",
    "            ds_coords = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_PARENT_GROUP)\n",
    "\n",
    "            lst_data = ds_data[HDF5_DEGISKENI]\n",
    "            \n",
    "            lst_data = lst_data.rename({lst_data.dims[0]: 'y', lst_data.dims[1]: 'x'})\n",
    "            lst_data = lst_data.assign_coords({\"y\": ds_coords['YDim'].values, \"x\": ds_coords['XDim'].values})\n",
    "            \n",
    "            if '_FillValue' in lst_data.attrs:\n",
    "                lst_data = lst_data.where(lst_data != lst_data.attrs['_FillValue'])\n",
    "            if 'scale_factor' in lst_data.attrs:\n",
    "                lst_data = lst_data * lst_data.attrs['scale_factor']\n",
    "            lst_celsius = lst_data - 273.15\n",
    "            \n",
    "            lst_celsius.rio.write_crs(SINUSOIDAL_PROJ, inplace=True)\n",
    "            \n",
    "            tarih_match = re.search(r'\\.A(\\d{7})\\.', str(dosya_path))\n",
    "            if not tarih_match: continue\n",
    "            tarih = pd.to_datetime(tarih_match.group(1), format='%Y%j').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(lst_celsius.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = lst_celsius.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_sicaklik = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_sicaklik):\n",
    "                        tum_sonuclar.append({\"sehir\": sehir_adi, \"tarih\": tarih, \"sicaklik_C\": ortalama_sicaklik})\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ HATA: '{dosya_path.name}' atlanÄ±yor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"\\nâŒ Test iÅŸlemi baÅŸarÄ±sÄ±z. HiÃ§bir veri iÅŸlenemedi.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTest verileri iÅŸlendi. CSV dosyasÄ± oluÅŸturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"âœ¨ Test BaÅŸarÄ±lÄ±! SonuÃ§lar '{CIKTI_CSV_DOSYASI}' dosyasÄ±na kaydedildi.\")\n",
    "    print(\"\\nCSV DosyasÄ±nÄ±n ilk 5 satÄ±rÄ±:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    skorlari_hesapla_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f7108",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# YaÄŸÄ±ÅŸ / Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00742a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneyi kuralÄ±m\n",
    "!pip install earthaccess -q\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR (YAÄIÅ VERÄ°SÄ° Ä°Ã‡Ä°N UYARLANDI) ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'GPM_3IMERGDF' # <-- GÃœNCELLENDÄ°\n",
    "VERSIYON = '07'                # <-- GÃœNCELLENDÄ°\n",
    "YIL = 2020                     # <-- Ä°ndirmek istediÄŸin yÄ±lÄ± buraya yaz\n",
    "\n",
    "# TÃ¼rkiye'yi iÃ§ine alan coÄŸrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceÄŸi klasÃ¶r\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAÅLANGICI (DEÄÄ°ÅÄ°KLÄ°K YOK) ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriÅŸ yapÄ±lÄ±yor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"âœ… GiriÅŸ baÅŸarÄ±lÄ±.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ GiriÅŸ baÅŸarÄ±sÄ±z. LÃ¼tfen .netrc dosyasÄ±nÄ± kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirilecek.\\n\")\n",
    "\n",
    "# BitiÅŸ tarihini YIL'a gÃ¶re doÄŸru ayarlayan mantÄ±k\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, YÄ±l: {YIL}\")\n",
    "print(f\"Zaman AralÄ±ÄŸÄ±: {baslangic_tarihi} -> {bitis_tarihi}\")\n",
    "print(f\"BÃ¶lge: TÃ¼rkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranÄ±yor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"âœ… {len(sonuclar)} adet dosya bulundu. Ä°ndirme iÅŸlemi baÅŸlÄ±yor...\")\n",
    "        print(\"â— Bu iÅŸlem biraz zaman alabilir, lÃ¼tfen sabÄ±rlÄ± olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\nâœ¨ Ä°ndirme tamamlandÄ±! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasÃ¶rÃ¼ne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš  Belirtilen tarih ve bÃ¶lge iÃ§in hiÃ§ dosya bulunamadÄ±.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Arama veya indirme sÄ±rasÄ±nda bir hata oluÅŸtu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dfb84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rioxarray\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from shapely.geometry import box\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve iÅŸlenecek dosya sayÄ±sÄ±nÄ± belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 10\n",
    "\n",
    "YIL = 2020\n",
    "\n",
    "VERI_KLASORU = f\"NASA_Verileri/GPM_3IMERGDF/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "\n",
    "# Ã‡Ä±ktÄ± dosya adÄ±nÄ± test moduna gÃ¶re ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_yagis_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_yagis_skorlari_{YIL}.csv\"\n",
    "\n",
    "YAGIS_DEGISKENI = 'precipitation'\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# --- TEK BÄ°R DOSYAYI Ä°ÅLEYEN FONKSÄ°YON ---\n",
    "def process_precipitation_file(dosya_path):\n",
    "    try:\n",
    "        ds = xr.open_dataset(dosya_path)\n",
    "        yagis_data = ds[YAGIS_DEGISKENI]\n",
    "\n",
    "        if 'time' in yagis_data.dims:\n",
    "            yagis_data = yagis_data.squeeze('time', drop=True)\n",
    "\n",
    "        yagis_data.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "        yagis_data.rio.write_crs(\"epsg:4236\", inplace=True)\n",
    "        \n",
    "        min_lon, min_lat, max_lon, max_lat = TURKIYE_BOUNDING_BOX\n",
    "        turkey_geom = box(min_lon, min_lat, max_lon, max_lat)\n",
    "        yagis_turkiye = yagis_data.rio.clip([turkey_geom], \"epsg:4236\")\n",
    "        \n",
    "        tarih_match = re.search(r'\\.(\\d{8})-S', str(dosya_path))\n",
    "        if not tarih_match: return []\n",
    "        tarih = pd.to_datetime(tarih_match.group(1), format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "        \n",
    "        iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "        iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "        iller_gdf_proj = iller_gdf.to_crs(yagis_data.rio.crs)\n",
    "        \n",
    "        gunluk_sonuclar = []\n",
    "        for index, il in iller_gdf_proj.iterrows():\n",
    "            sehir_adi = il['sehir']\n",
    "            try:\n",
    "                clipped = yagis_turkiye.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                ortalama_yagis = float(clipped.mean())\n",
    "                \n",
    "                if pd.notna(ortalama_yagis):\n",
    "                    gunluk_sonuclar.append({\n",
    "                        \"sehir\": sehir_adi,\n",
    "                        \"tarih\": tarih,\n",
    "                        \"yagis_mm_gun\": ortalama_yagis\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        return gunluk_sonuclar\n",
    "    except Exception as e:\n",
    "        # print(f\"HATA: {dosya_path.name} iÅŸlenemedi. Sebep: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- ANA KOD ---\n",
    "if __name__ == '__main__':\n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    nc4_dosyalari = sorted(list(veri_yolu.glob('*.nc4')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kÄ±salt\n",
    "    if TEST_MODU:\n",
    "        nc4_dosyalari = nc4_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"ğŸ TEST MODU AKTÄ°F: Sadece ilk {len(nc4_dosyalari)} dosya iÅŸlenecek.\")\n",
    "\n",
    "    if not nc4_dosyalari:\n",
    "        print(f\"âŒ HATA: '{veri_yolu}' klasÃ¶rÃ¼nde .nc4 dosyasÄ± bulunamadÄ±.\")\n",
    "    else:\n",
    "        print(f\"Ä°ÅŸlenmek Ã¼zere {len(nc4_dosyalari)} adet yaÄŸÄ±ÅŸ dosyasÄ± seÃ§ildi.\")\n",
    "        print(\"Optimize edilmiÅŸ paralel iÅŸleme baÅŸlÄ±yor...\")\n",
    "\n",
    "        tum_sonuclar = []\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results_iterator = list(tqdm(executor.map(process_precipitation_file, nc4_dosyalari), total=len(nc4_dosyalari)))\n",
    "        \n",
    "        for result in results_iterator:\n",
    "            tum_sonuclar.extend(result)\n",
    "\n",
    "        if not tum_sonuclar:\n",
    "            print(\"\\nâŒ Ä°ÅŸlem bitti ancak hiÃ§bir veri iÅŸlenemedi.\")\n",
    "        else:\n",
    "            print(\"\\nTest verileri iÅŸlendi. CSV dosyasÄ± oluÅŸturuluyor...\")\n",
    "            sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "            sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "            print(f\"âœ¨ Test BaÅŸarÄ±lÄ±! SonuÃ§lar '{CIKTI_CSV_DOSYASI}' dosyasÄ±na kaydedildi.\")\n",
    "            print(\"\\nCSV DosyasÄ±nÄ±n ilk 5 satÄ±rÄ±:\")\n",
    "            print(sonuc_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c003285",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model EÄŸitimi / Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ed185",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas sklearn prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66413457",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prophet import Prophet\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os # Dosya yollarÄ±nÄ± birleÅŸtirmek iÃ§in eklendi\n",
    "\n",
    "# Prophet ve Stan'den gelen gereksiz log mesajlarÄ±nÄ± kapatalÄ±m\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# DEÄÄ°ÅÄ°KLÄ°K: ArtÄ±k birden fazla yÄ±lÄ± iÅŸliyoruz\n",
    "YILLAR = [2020, 2021, 2022, 2023, 2024,2025] \n",
    "ANA_KLASOR = \"/content/drive/MyDrive/Nasa/Tum_veri/\" # Ana dosya yolu\n",
    "\n",
    "# Tahmin edilecek Ã¶zel tarihler\n",
    "TAHMIN_TARIHLERI = ['2027-01-15', '2027-04-15', '2027-07-15', '2027-10-15']\n",
    "\n",
    "# Ã‡Ä±ktÄ± dosyalarÄ±nÄ±n adlarÄ±\n",
    "YIL_ARALIGI_STR = f\"{YILLAR[0]}-{YILLAR[-1]}\"\n",
    "FINAL_DAILY_CSV = f\"EcoPulse_Skorlari_GUNLUK_{YIL_ARALIGI_STR}_Temizlenmis.csv\"\n",
    "FINAL_MONTHLY_CSV = f\"EcoPulse_Skorlari_AYLIK_{YIL_ARALIGI_STR}_Temizlenmis.csv\" # YENÄ° Ã‡IKTI\n",
    "FINAL_FORECAST_CSV = f\"EcoPulse_2027_Tahminleri_{YIL_ARALIGI_STR}_Verisiyle.csv\"\n",
    "\n",
    "# --- 1. AdÄ±m: TÃ¼m YÄ±llar Ä°Ã§in Veri HazÄ±rlama ve TEMÄ°ZLEME ---\n",
    "def cok_yilli_veri_hazirla():\n",
    "    print(f\"{YIL_ARALIGI_STR} yÄ±llarÄ± arasÄ±ndaki tÃ¼m veriler birleÅŸtiriliyor...\")\n",
    "    \n",
    "    tum_datalar = []\n",
    "    for yil in YILLAR:\n",
    "        try:\n",
    "            # Her yÄ±l iÃ§in dosya yollarÄ±nÄ± dinamik olarak oluÅŸtur\n",
    "            no2_csv = os.path.join(ANA_KLASOR, f\"hava_kirliligi/sehir_bazli_no2_skorlari_{yil}.csv\")\n",
    "            gece_temp_csv = os.path.join(ANA_KLASOR, f\"sÄ±caklÄ±k_gece/sehir_bazli_sicaklik_skorlari_{yil}.csv\")\n",
    "            gunduz_temp_csv = os.path.join(ANA_KLASOR, f\"sÄ±caklÄ±k_gunduz/sehir_bazli_sicaklik_skorlari_{yil}_gunduz.csv\")\n",
    "            yagis_csv = os.path.join(ANA_KLASOR, f\"yagÄ±s/sehir_bazli_yagis_skorlari_{yil}.csv\")\n",
    "\n",
    "            df_no2 = pd.read_csv(no2_csv)\n",
    "            df_gece_temp = pd.read_csv(gece_temp_csv)\n",
    "            df_gunduz_temp = pd.read_csv(gunduz_temp_csv)\n",
    "            df_yagis = pd.read_csv(yagis_csv)\n",
    "            \n",
    "            df_gece_temp = df_gece_temp.rename(columns={'sicaklik_C': 'sicaklik_gece_C'})\n",
    "            df_gunduz_temp = df_gunduz_temp.rename(columns={'sicaklik_C': 'sicaklik_gunduz_C'})\n",
    "            \n",
    "            df_merged = pd.merge(df_no2, df_gece_temp, on=['sehir', 'tarih'])\n",
    "            df_merged = pd.merge(df_merged, df_gunduz_temp, on=['sehir', 'tarih'])\n",
    "            df_merged = pd.merge(df_merged, df_yagis, on=['sehir', 'tarih'])\n",
    "            \n",
    "            tum_datalar.append(df_merged)\n",
    "            print(f\"âœ… {yil} yÄ±lÄ± verileri baÅŸarÄ±yla okundu.\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"âŒ UYARI: {yil} yÄ±lÄ± iÃ§in bir veya daha fazla dosya bulunamadÄ±. Bu yÄ±l atlanÄ±yor. Hata: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_datalar:\n",
    "        print(\"âŒ HATA: HiÃ§bir yÄ±la ait veri okunamadÄ±. LÃ¼tfen dosya yollarÄ±nÄ± ve adlarÄ±nÄ± kontrol edin.\")\n",
    "        return None\n",
    "\n",
    "    # TÃ¼m yÄ±llarÄ±n verilerini tek bir dataframe'de birleÅŸtir\n",
    "    df_final = pd.concat(tum_datalar, ignore_index=True)\n",
    "    df_final['tarih'] = pd.to_datetime(df_final['tarih'])\n",
    "    \n",
    "    # --- VERÄ° TEMÄ°ZLEME ADIMI ---\n",
    "    print(\"\\nVeri temizleme adÄ±mÄ± baÅŸlatÄ±lÄ±yor...\")\n",
    "    orijinal_satir_sayisi = len(df_final)\n",
    "    MIN_SICAKLIK, MAX_SICAKLIK = -25.0, 55.0\n",
    "    \n",
    "    df_final = df_final[\n",
    "        (df_final['sicaklik_gece_C'].between(MIN_SICAKLIK, MAX_SICAKLIK)) &\n",
    "        (df_final['sicaklik_gunduz_C'].between(MIN_SICAKLIK, MAX_SICAKLIK))\n",
    "    ]\n",
    "    \n",
    "    kaldirilan_satir_sayisi = orijinal_satir_sayisi - len(df_final)\n",
    "    print(f\"âœ… Veri temizleme tamamlandÄ±. {kaldirilan_satir_sayisi} adet hatalÄ± satÄ±r kaldÄ±rÄ±ldÄ±.\")\n",
    "    \n",
    "    # --- GÃœNLÃœK ECOPULSE HESAPLAMA ---\n",
    "    print(\"\\nGÃ¼nlÃ¼k EcoPulse skorlarÄ± hesaplanÄ±yor...\")\n",
    "    df_final['sicaklik_farki_DTR'] = df_final['sicaklik_gunduz_C'] - df_final['sicaklik_gece_C']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_final['P_norm'] = (1 - scaler.fit_transform(df_final[['no2_skoru']])) * 100\n",
    "    df_final['H_norm'] = (1 - scaler.fit_transform(df_final[['sicaklik_gece_C']])) * 100\n",
    "    df_final['R_norm'] = scaler.fit_transform(df_final[['yagis_mm_gun']]) * 100\n",
    "    df_final['DTR_norm'] = (1 - scaler.fit_transform(df_final[['sicaklik_farki_DTR']])) * 100\n",
    "    \n",
    "    df_final['EcoPulse'] = (0.30 * df_final['P_norm']) + (0.30 * df_final['DTR_norm']) + \\\n",
    "                           (0.20 * df_final['R_norm']) + (0.20 * df_final['H_norm'])\n",
    "    print(\"âœ… GÃ¼nlÃ¼k EcoPulse skorlarÄ± hazÄ±r.\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# --- Model Fonksiyonu (DeÄŸiÅŸiklik yok) ---\n",
    "def train_and_forecast_city_specific_dates(args):\n",
    "    # ... (Bu fonksiyon Ã¶nceki kod ile aynÄ±, deÄŸiÅŸiklik yok)\n",
    "    sehir_adi, df_sehir, dates_to_predict = args\n",
    "    try:\n",
    "        if len(df_sehir) < 2: return []\n",
    "        df_prophet = df_sehir[['tarih', 'EcoPulse']].rename(columns={'tarih': 'ds', 'EcoPulse': 'y'})\n",
    "        df_prophet['cap'], df_prophet['floor'] = 100, 0\n",
    "        model = Prophet(growth='logistic', yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df_prophet)\n",
    "        future = pd.DataFrame({'ds': dates_to_predict}); future['cap'], future['floor'] = 100, 0\n",
    "        forecast = model.predict(future)\n",
    "        return [{\"sehir\": sehir_adi, \"tahmin_tarihi\": row['ds'].strftime('%Y-%m-%d'), \"tahmini_ecopulse_skoru\": row['yhat']} for _, row in forecast.iterrows()]\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: {sehir_adi} ÅŸehri iÃ§in tahmin yapÄ±lÄ±rken bir sorun oluÅŸtu: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- ANA KOD ---\n",
    "if __name__ == '__main__':\n",
    "    df_gunluk_ecopulse = cok_yilli_veri_hazirla()\n",
    "    \n",
    "    if df_gunluk_ecopulse is not None and not df_gunluk_ecopulse.empty:\n",
    "        # --- GÃœNLÃœK VERÄ°YÄ° KAYDET ---\n",
    "        print(f\"\\nGÃ¼nlÃ¼k hesaplanan EcoPulse skorlarÄ± '{FINAL_DAILY_CSV}' dosyasÄ±na kaydediliyor...\")\n",
    "        kolonlar = ['sehir', 'tarih', 'EcoPulse', 'no2_skoru', 'sicaklik_gece_C', 'sicaklik_gunduz_C', 'yagis_mm_gun']\n",
    "        df_gunluk_ecopulse[kolonlar].to_csv(FINAL_DAILY_CSV, index=False, encoding='utf-8', float_format='%.2f')\n",
    "        print(f\"âœ… GÃ¼nlÃ¼k veriler baÅŸarÄ±yla kaydedildi.\")\n",
    "\n",
    "        # --- YENÄ° ADIM: AYLIK ORTALAMALARI HESAPLA VE KAYDET ---\n",
    "        print(f\"\\nAylÄ±k ortalama EcoPulse skorlarÄ± '{FINAL_MONTHLY_CSV}' dosyasÄ±na kaydediliyor...\")\n",
    "        df_aylik = df_gunluk_ecopulse.groupby(['sehir', pd.Grouper(key='tarih', freq='M')])['EcoPulse'].mean().reset_index()\n",
    "        df_aylik = df_aylik.rename(columns={'EcoPulse': 'aylik_ortalama_ecopulse'})\n",
    "        df_aylik['tarih'] = df_aylik['tarih'].dt.to_period('M')\n",
    "        df_aylik['aylik_ortalama_ecopulse'] = df_aylik['aylik_ortalama_ecopulse'].round(2)\n",
    "        df_aylik.to_csv(FINAL_MONTHLY_CSV, index=False, encoding='utf-8')\n",
    "        print(f\"âœ… AylÄ±k ortalama verileri baÅŸarÄ±yla kaydedildi.\")\n",
    "        print(\"AylÄ±k verilerden Ã¶rnek:\")\n",
    "        print(df_aylik.head())\n",
    "        # --- AYLIK ORTALAMA ADIMI BÄ°TTÄ° ---\n",
    "\n",
    "        # --- MODEL EÄÄ°TÄ°MÄ° VE TAHMÄ°N ---\n",
    "        predict_dates = pd.to_datetime(TAHMIN_TARIHLERI)\n",
    "        sehirler = df_gunluk_ecopulse['sehir'].unique()\n",
    "        sehir_datalari = [(sehir, df_gunluk_ecopulse[df_gunluk_ecopulse['sehir'] == sehir], predict_dates) for sehir in sehirler]\n",
    "        \n",
    "        print(f\"\\n{len(sehirler)} ÅŸehir iÃ§in paralel olarak {len(predict_dates)} Ã¶zel gÃ¼n tahmini yapÄ±lÄ±yor...\")\n",
    "        \n",
    "        tum_tahminler = []\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(train_and_forecast_city_specific_dates, sehir_datalari), total=len(sehir_datalari)))\n",
    "\n",
    "        for res in results: tum_tahminler.extend(res)\n",
    "        \n",
    "        if tum_tahminler:\n",
    "            print(\"\\nâœ… TÃ¼m ÅŸehirler iÃ§in Ã¶zel gÃ¼n tahminleri tamamlandÄ±.\")\n",
    "            df_tahmin = pd.DataFrame(tum_tahminler)\n",
    "            df_tahmin['tahmini_ecopulse_skoru'] = df_tahmin['tahmini_ecopulse_skoru'].clip(0, 100).round(2)\n",
    "            df_tahmin.to_csv(FINAL_FORECAST_CSV, index=False, encoding='utf-8')\n",
    "            print(f\"âœ¨ Tahmin sonuÃ§larÄ± '{FINAL_FORECAST_CSV}' dosyasÄ±na kaydedildi.\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Tahmin Ã¼retilemedi.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ HiÃ§ geÃ§erli veri kalmadÄ±ÄŸÄ± iÃ§in hiÃ§bir iÅŸlem yapÄ±lamadÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52c7c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# LÃ¼tfen ana kodun Ã¼rettiÄŸi AYLIK ortalama dosyasÄ±nÄ±n adÄ±nÄ±n\n",
    "# bu olduÄŸundan emin ol. YÄ±l aralÄ±ÄŸÄ± farklÄ±ysa (Ã¶rn: 2020-2024),\n",
    "# bu deÄŸiÅŸkeni gÃ¼ncelleyebilirsin.\n",
    "YIL_ARALIGI_STR = \"2020-2025\" \n",
    "AYLIK_ORTALAMA_DOSYASI = f\"EcoPulse_Skorlari_AYLIK_{YIL_ARALIGI_STR}_Temizlenmis.csv\"\n",
    "\n",
    "def kategori_sinirlarini_bul():\n",
    "    \"\"\"\n",
    "    AylÄ±k EcoPulse skorlarÄ±nÄ± analiz eder ve web sitesinde kullanÄ±lacak\n",
    "    KÄ±rmÄ±zÄ±, SarÄ±, YeÅŸil renk kategorileri iÃ§in en uygun sÄ±nÄ±rlarÄ± Ã¶nerir.\n",
    "    \"\"\"\n",
    "    # DosyanÄ±n var olup olmadÄ±ÄŸÄ±nÄ± kontrol et\n",
    "    if not os.path.exists(AYLIK_ORTALAMA_DOSYASI):\n",
    "        print(f\"âŒ HATA: '{AYLIK_ORTALAMA_DOSYASI}' dosyasÄ± bulunamadÄ±.\")\n",
    "        print(\"LÃ¼tfen dosya adÄ±nÄ±n doÄŸru olduÄŸundan ve bu kodun ilgili dosyayla aynÄ± klasÃ¶rde olduÄŸundan emin ol.\")\n",
    "        return\n",
    "\n",
    "    print(f\"'{AYLIK_ORTALAMA_DOSYASI}' dosyasÄ± okunuyor ve analiz ediliyor...\")\n",
    "    \n",
    "    # 1. AdÄ±m: Veriyi oku\n",
    "    df_aylik = pd.read_csv(AYLIK_ORTALAMA_DOSYASI)\n",
    "    skor_kolonu = 'aylik_ortalama_ecopulse'\n",
    "\n",
    "    # 2. AdÄ±m: Verinin genel istatistiksel daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster\n",
    "    # Bu, verinin genel yapÄ±sÄ±nÄ± anlamamÄ±zÄ± saÄŸlar (min, max, ortalama vb.)\n",
    "    print(\"\\n--- AylÄ±k EcoPulse SkorlarÄ±nÄ±n Genel DaÄŸÄ±lÄ±mÄ± ---\")\n",
    "    distribution_stats = df_aylik[skor_kolonu].describe()\n",
    "    print(distribution_stats)\n",
    "    \n",
    "    # 3. AdÄ±m: YÃ¼zdelik dilim (percentile) yÃ¶ntemini kullanarak sÄ±nÄ±rlarÄ± hesapla\n",
    "    # Bu yÃ¶ntem, veri setini 3 eÅŸit parÃ§aya bÃ¶ler.\n",
    "    # Her renk kategorisinin veri setinde anlamlÄ± sayÄ±da Ã¶rnek iÃ§ermesini saÄŸlar.\n",
    "    try:\n",
    "        sinir_kirmizi_sari = np.percentile(df_aylik[skor_kolonu], 33)\n",
    "        sinir_sari_yesil = np.percentile(df_aylik[skor_kolonu], 66)\n",
    "        \n",
    "        min_skor = distribution_stats['min']\n",
    "        max_skor = distribution_stats['max']\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“Š WEB SÄ°TESÄ° Ä°Ã‡Ä°N RENK KATEGORÄ°SÄ° Ã–NERÄ°LERÄ° ğŸ“Š\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nSenin ilk Ã¶nerin:\")\n",
    "        print(f\"ğŸ”´ KÄ±rmÄ±zÄ±: 0 - 35\")\n",
    "        print(f\"ğŸŸ¡ SarÄ±   : 35 - 65\")\n",
    "        print(f\"ğŸŸ¢ YeÅŸil  : 65 - 100\")\n",
    "        \n",
    "        # SonuÃ§larÄ± daha temiz ve kullanÄ±labilir hale getirelim (tam sayÄ±lara yuvarlayarak)\n",
    "        alt_sinir_sari = int(round(sinir_kirmizi_sari))\n",
    "        alt_sinir_yesil = int(round(sinir_sari_yesil))\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Veri DaÄŸÄ±lÄ±mÄ±na GÃ¶re Ã–nerilen Yeni AralÄ±klar:\")\n",
    "        print(f\"ğŸ”´ KÄ±rmÄ±zÄ± (DÃ¼ÅŸÃ¼k Performans): 0 - {alt_sinir_sari}\")\n",
    "        print(f\"ğŸŸ¡ SarÄ±   (Orta Performans)  : {alt_sinir_sari} - {alt_sinir_yesil}\")\n",
    "        print(f\"ğŸŸ¢ YeÅŸil  (Ä°yi Performans)   : {alt_sinir_yesil} - 100\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nBu aralÄ±klar, veri setindeki skorlarÄ± yaklaÅŸÄ±k 3 eÅŸit gruba ayÄ±rÄ±r.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nHesaplama sÄ±rasÄ±nda bir hata oluÅŸtu: {e}\")\n",
    "\n",
    "# Ana fonksiyonu Ã§alÄ±ÅŸtÄ±r\n",
    "if __name__ == '__main__':\n",
    "    kategori_sinirlarini_bul()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
