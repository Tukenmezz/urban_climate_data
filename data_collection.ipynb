{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "169a28be",
   "metadata": {},
   "source": [
    "# Hava Kirliliği / Air Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa610f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneyi kuralım\n",
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Hangi yılları indirmek istiyoruz?\n",
    "HEDEFLENEN_YILLAR = [2020,2021,2022,2023,2024,2025]\n",
    "\n",
    "# Hangi veri setlerini indireceğiz? (Daha önce test ettiklerimiz)\n",
    "# ('NASA Veri Seti Kısa Adı', 'Versiyon Numarası')\n",
    "VERI_SETLERI = [\n",
    "    ('OMNO2d', '003'),                          # OMI-Aura NO₂ verisi (.he5)\n",
    "]\n",
    "\n",
    "# Verilerin indirileceği ana klasör\n",
    "INDIRME_KLASORU = \"NASA_Verileri_2\"\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriş yapılıyor...\")\n",
    "try:\n",
    "    earthaccess.login()\n",
    "    print(\"✅ Giriş başarılı.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Giriş başarısız. Lütfen yukarıdaki .netrc oluşturma adımını kontrol edin. Hata: {e}\")\n",
    "    # Giriş başarısız olursa kodu burada durdur.\n",
    "    exit()\n",
    "\n",
    "# Ana indirme klasörünü oluştur\n",
    "Path(INDIRME_KLASORU).mkdir(exist_ok=True)\n",
    "\n",
    "# Bugünün tarihini al (2025 yılı için bitiş tarihi olarak kullanılacak)\n",
    "bugun = datetime.now()\n",
    "\n",
    "for short_name, version in VERI_SETLERI:\n",
    "    for yil in HEDEFLENEN_YILLAR:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"İşlem başlıyor: Veri Seti='{short_name}', Yıl='{yil}'\")\n",
    "        \n",
    "        # İndirme için alt klasör oluştur (örn: NASA_Verileri/OMNO2d/2020)\n",
    "        hedef_klasor = Path(INDIRME_KLASORU) / short_name / str(yil)\n",
    "        hedef_klasor.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Dosyalar '{hedef_klasor}' klasörüne indirilecek.\")\n",
    "\n",
    "        # Zaman aralığını belirle\n",
    "        baslangic_tarihi = f\"{yil}-01-01\"\n",
    "        if yil == bugun.year:\n",
    "            # Eğer yıl 2025 ise, bitiş tarihi bugün olsun\n",
    "            bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "        else:\n",
    "            # Diğer yıllar için tüm yılı kapsa\n",
    "            bitis_tarihi = f\"{yil}-12-31\"\n",
    "            \n",
    "        print(f\"Zaman Aralığı: {baslangic_tarihi} -> {bitis_tarihi}\")\n",
    "\n",
    "        # NASA sunucularında veri arama\n",
    "        try:\n",
    "            sonuclar = earthaccess.search_data(\n",
    "                short_name=short_name,\n",
    "                version=version,\n",
    "                temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "            )\n",
    "            \n",
    "            if not sonuclar:\n",
    "                print(\"⚠️ Bu tarih aralığı için hiç dosya bulunamadı.\")\n",
    "                continue # Bir sonraki yıla veya veri setine geç\n",
    "\n",
    "            print(f\"✅ {len(sonuclar)} adet dosya bulundu. İndirme işlemi başlıyor...\")\n",
    "            \n",
    "            # Bulunan dosyaları indir\n",
    "            earthaccess.download(sonuclar, local_path=str(hedef_klasor))\n",
    "            print(f\"✨ '{short_name}' için {yil} yılı verileri başarıyla indirildi.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ '{short_name}' için {yil} yılında veri aranırken/indirilirken bir hata oluştu: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tüm indirme işlemleri tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97027e40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rioxarray\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np # Koordinatları oluşturmak için eklendi\n",
    "\n",
    "# --- AYARLAR BÖLÜMÜ ---\n",
    "VERI_KLASORU = \"NASA_Verileri/OMNO2d/2020\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "CIKTI_CSV_DOSYASI = \"sehir_bazli_no2_skorlari_2020.csv\"\n",
    "\n",
    "# Verinin bulunduğu tek grup yolu\n",
    "HDF5_DATA_GROUP = 'HDFEOS/GRIDS/ColumnAmountNO2/Data Fields'\n",
    "\n",
    "DEGISKEN_ADLARI_LISTESI = [\n",
    "    'ColumnAmountNO2TropCloudScreened',\n",
    "    'ColumnAmountNO2Trop'\n",
    "]\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "def final_skorlari_hesapla():\n",
    "    print(f\"Yerel '{GEOJSON_DOSYASI}' dosyasından il sınırları okunuyor...\")\n",
    "    try:\n",
    "        iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "        iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "        print(f\"✅ {len(iller_gdf)} adet il başarıyla yüklendi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ HATA: '{GEOJSON_DOSYASI}' dosyası okunamadı. -> {e}\")\n",
    "        return\n",
    "\n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    he5_dosyalari = sorted(list(veri_yolu.glob('*.he5')))\n",
    "    if not he5_dosyalari:\n",
    "        print(f\"❌ HATA: '{veri_yolu}' klasöründe dosya bulunamadı.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"İşlenmek üzere {len(he5_dosyalari)} adet dosya bulundu.\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(he5_dosyalari, desc=\"Günlük Veriler İşleniyor\"):\n",
    "        try:\n",
    "            # === YENİ VE KESİN YÖNTEM ===\n",
    "            # 1. Sadece veri içeren grubu aç\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_DATA_GROUP)\n",
    "            \n",
    "            # 2. Akıllı değişken seçimi\n",
    "            kullanilacak_degisken = next((var for var in DEGISKEN_ADLARI_LISTESI if var in ds_data.data_vars), None)\n",
    "            if not kullanilacak_degisken:\n",
    "                continue\n",
    "\n",
    "            rds = ds_data[kullanilacak_degisken]\n",
    "\n",
    "            # 3. Koordinatları sıfırdan oluştur ve veriye ata\n",
    "            # Boyutları yeniden adlandır\n",
    "            rds = rds.rename({rds.dims[0]: 'y', rds.dims[1]: 'x'})\n",
    "            \n",
    "            # Global grid için koordinat dizilerini oluştur\n",
    "            # 1440 piksel -180'den +180'e (boylam), 720 piksel +90'dan -90'a (enlem)\n",
    "            lon_coords = np.linspace(-180 + (0.25/2), 180 - (0.25/2), 1440)\n",
    "            lat_coords = np.linspace(90 - (0.25/2), -90 + (0.25/2), 720)\n",
    "            \n",
    "            # Oluşturulan koordinatları veriye ata\n",
    "            rds = rds.assign_coords(x=lon_coords, y=lat_coords)\n",
    "            \n",
    "            # Coğrafi referans sistemini belirt\n",
    "            rds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "            # === ESKİ KOD DEVAM EDİYOR ===\n",
    "            tarih_str_match = re.search(r'_(\\d{4}m\\d{4})_', str(dosya_path))\n",
    "            if not tarih_str_match: continue\n",
    "            tarih = pd.to_datetime(tarih_str_match.group(1).replace('m', '-'), format='%Y-%m%d').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(rds.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = rds.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_skor = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_skor) and ortalama_skor > 0:\n",
    "                        tum_sonuclar.append({\n",
    "                            \"sehir\": sehir_adi,\n",
    "                            \"tarih\": tarih,\n",
    "                            \"no2_skoru\": ortalama_skor\n",
    "                        })\n",
    "                except Exception:\n",
    "                    # Bu şehir için veri bulunamazsa (örn. okyanusa denk gelirse) atla\n",
    "                    continue\n",
    "        except Exception:\n",
    "            # print(f\"\\n⚠️ HATA: '{dosya_path.name}' işlenirken genel bir sorun oluştu. Atlanıyor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"❌ Üzgünüm, yine hiçbir dosya başarıyla işlenemedi. Veri yapısında beklenmedik bir durum olabilir.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTüm veriler işlendi. CSV dosyası oluşturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df['no2_skoru'] = sonuc_df['no2_skoru'] * 1e15\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"✨ Başarılı! Sonuçlar '{CIKTI_CSV_DOSYASI}' dosyasına kaydedildi.\")\n",
    "    print(\"\\nCSV Dosyasının ilk 5 satırı:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_skorlari_hesapla()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03f464",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Sıcaklık Gündüz / Temperature Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31aa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'VNP21A1D'\n",
    "VERSIYON = '002'\n",
    "YIL = 2021 # <-- Artık buraya yazdığın yıl ne ise, sadece o indirilecek.\n",
    "\n",
    "# Türkiye'yi içine alan coğrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceği klasör\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriş yapılıyor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"✅ Giriş başarılı.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Giriş başarısız. Lütfen .netrc dosyasını kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasörüne indirilecek.\\n\")\n",
    "\n",
    "# === DÜZELTME BURADA: Bitiş tarihini YIL'a göre doğru ayarlayan mantık ===\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    # Eğer geçmiş bir yılı arıyorsak, bitiş tarihi o yılın sonu (31 Aralık) olmalı.\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    # Eğer içinde bulunduğumuz yılı (2025) arıyorsak, bitiş tarihi bugün olsun.\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "# === DÜZELTME SONU ===\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, Yıl: {YIL}\")\n",
    "print(f\"Zaman Aralığı: {baslangic_tarihi} -> {bitis_tarihi}\") # Bu satır artık doğru aralığı gösterecek\n",
    "print(f\"Bölge: Türkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranıyor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"✅ {len(sonuclar)} adet dosya bulundu. İndirme işlemi başlıyor...\")\n",
    "        print(\"❗ Bu işlem uzun sürebilir, lütfen sabırlı olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\n✨ İndirme tamamlandı! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasörüne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠ Belirtilen tarih ve bölge için hiç dosya bulunamadı.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Arama veya indirme sırasında bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecff740",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve işlenecek dosya sayısını belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 50 # Hızlı bir test için sadece 5 dosya\n",
    "\n",
    "YIL = 2021\n",
    "VERI_KLASORU = f\"NASA_Verileri/VNP21A1D/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "# Çıktı dosya adını test moduna göre ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_gunduz.csv\"\n",
    "\n",
    "# h5 dosyası içindeki doğru grup ve değişken adları\n",
    "HDF5_PARENT_GROUP = 'HDFEOS/GRIDS/VIIRS_Grid_Daily_1km_LST21'\n",
    "HDF5_DATA_SUBGROUP = 'Data Fields'\n",
    "HDF5_DEGISKENI = 'LST_1KM'\n",
    "\n",
    "# NASA VIIRS/MODIS verilerinin kullandığı standart Sinusoidal Projeksiyon\n",
    "SINUSOIDAL_PROJ = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "def skorlari_hesapla_test():\n",
    "    iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "    iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "    \n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    h5_dosyalari = sorted(list(veri_yolu.glob('*.h5')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kısalt\n",
    "    if TEST_MODU:\n",
    "        h5_dosyalari = h5_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"🐞 TEST MODU AKTİF: Sadece ilk {len(h5_dosyalari)} dosya işlenecek.\")\n",
    "    \n",
    "    print(f\"İşlenmek üzere {len(h5_dosyalari)} adet sıcaklık dosyası seçildi.\")\n",
    "    print(\"İşlem başlıyor...\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(h5_dosyalari, desc=\"Günlük Sıcaklık Verileri İşleniyor\"):\n",
    "        try:\n",
    "            data_group_path = f\"{HDF5_PARENT_GROUP}/{HDF5_DATA_SUBGROUP}\"\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=data_group_path)\n",
    "            ds_coords = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_PARENT_GROUP)\n",
    "\n",
    "            lst_data = ds_data[HDF5_DEGISKENI]\n",
    "            \n",
    "            lst_data = lst_data.rename({lst_data.dims[0]: 'y', lst_data.dims[1]: 'x'})\n",
    "            lst_data = lst_data.assign_coords({\"y\": ds_coords['YDim'].values, \"x\": ds_coords['XDim'].values})\n",
    "            \n",
    "            if '_FillValue' in lst_data.attrs:\n",
    "                lst_data = lst_data.where(lst_data != lst_data.attrs['_FillValue'])\n",
    "            if 'scale_factor' in lst_data.attrs:\n",
    "                lst_data = lst_data * lst_data.attrs['scale_factor']\n",
    "            lst_celsius = lst_data - 273.15\n",
    "            \n",
    "            lst_celsius.rio.write_crs(SINUSOIDAL_PROJ, inplace=True)\n",
    "            \n",
    "            tarih_match = re.search(r'\\.A(\\d{7})\\.', str(dosya_path))\n",
    "            if not tarih_match: continue\n",
    "            tarih = pd.to_datetime(tarih_match.group(1), format='%Y%j').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(lst_celsius.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = lst_celsius.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_sicaklik = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_sicaklik):\n",
    "                        tum_sonuclar.append({\"sehir\": sehir_adi, \"tarih\": tarih, \"sicaklik_C\": ortalama_sicaklik})\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ HATA: '{dosya_path.name}' atlanıyor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"\\n❌ Test işlemi başarısız. Hiçbir veri işlenemedi.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTest verileri işlendi. CSV dosyası oluşturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"✨ Test Başarılı! Sonuçlar '{CIKTI_CSV_DOSYASI}' dosyasına kaydedildi.\")\n",
    "    print(\"\\nCSV Dosyasının ilk 5 satırı:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    skorlari_hesapla_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd04e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Sıcaklık Gece / Tempareture Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d29264",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'VNP21A1N'\n",
    "VERSIYON = '002'\n",
    "YIL = 2021 # <-- Artık buraya yazdığın yıl ne ise, sadece o indirilecek.\n",
    "\n",
    "# Türkiye'yi içine alan coğrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceği klasör\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriş yapılıyor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"✅ Giriş başarılı.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Giriş başarısız. Lütfen .netrc dosyasını kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasörüne indirilecek.\\n\")\n",
    "\n",
    "# === DÜZELTME BURADA: Bitiş tarihini YIL'a göre doğru ayarlayan mantık ===\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    # Eğer geçmiş bir yılı arıyorsak, bitiş tarihi o yılın sonu (31 Aralık) olmalı.\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    # Eğer içinde bulunduğumuz yılı (2025) arıyorsak, bitiş tarihi bugün olsun.\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "# === DÜZELTME SONU ===\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, Yıl: {YIL}\")\n",
    "print(f\"Zaman Aralığı: {baslangic_tarihi} -> {bitis_tarihi}\") # Bu satır artık doğru aralığı gösterecek\n",
    "print(f\"Bölge: Türkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranıyor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"✅ {len(sonuclar)} adet dosya bulundu. İndirme işlemi başlıyor...\")\n",
    "        print(\"❗ Bu işlem uzun sürebilir, lütfen sabırlı olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\n✨ İndirme tamamlandı! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasörüne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠ Belirtilen tarih ve bölge için hiç dosya bulunamadı.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Arama veya indirme sırasında bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cce1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve işlenecek dosya sayısını belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 50 # Hızlı bir test için sadece 5 dosya\n",
    "\n",
    "YIL = 2021\n",
    "VERI_KLASORU = f\"NASA_Verileri/VNP21A1N/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "# Çıktı dosya adını test moduna göre ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_sicaklik_skorlari_{YIL}.csv\"\n",
    "\n",
    "# h5 dosyası içindeki doğru grup ve değişken adları\n",
    "HDF5_PARENT_GROUP = 'HDFEOS/GRIDS/VIIRS_Grid_Daily_1km_LST21'\n",
    "HDF5_DATA_SUBGROUP = 'Data Fields'\n",
    "HDF5_DEGISKENI = 'LST_1KM'\n",
    "\n",
    "# NASA VIIRS/MODIS verilerinin kullandığı standart Sinusoidal Projeksiyon\n",
    "SINUSOIDAL_PROJ = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "\n",
    "# --- KOD BAŞLANGICI ---\n",
    "\n",
    "def skorlari_hesapla_test():\n",
    "    iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "    iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "    \n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    h5_dosyalari = sorted(list(veri_yolu.glob('*.h5')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kısalt\n",
    "    if TEST_MODU:\n",
    "        h5_dosyalari = h5_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"🐞 TEST MODU AKTİF: Sadece ilk {len(h5_dosyalari)} dosya işlenecek.\")\n",
    "    \n",
    "    print(f\"İşlenmek üzere {len(h5_dosyalari)} adet sıcaklık dosyası seçildi.\")\n",
    "    print(\"İşlem başlıyor...\")\n",
    "\n",
    "    tum_sonuclar = []\n",
    "\n",
    "    for dosya_path in tqdm(h5_dosyalari, desc=\"Günlük Sıcaklık Verileri İşleniyor\"):\n",
    "        try:\n",
    "            data_group_path = f\"{HDF5_PARENT_GROUP}/{HDF5_DATA_SUBGROUP}\"\n",
    "            ds_data = xr.open_dataset(dosya_path, engine='h5netcdf', group=data_group_path)\n",
    "            ds_coords = xr.open_dataset(dosya_path, engine='h5netcdf', group=HDF5_PARENT_GROUP)\n",
    "\n",
    "            lst_data = ds_data[HDF5_DEGISKENI]\n",
    "            \n",
    "            lst_data = lst_data.rename({lst_data.dims[0]: 'y', lst_data.dims[1]: 'x'})\n",
    "            lst_data = lst_data.assign_coords({\"y\": ds_coords['YDim'].values, \"x\": ds_coords['XDim'].values})\n",
    "            \n",
    "            if '_FillValue' in lst_data.attrs:\n",
    "                lst_data = lst_data.where(lst_data != lst_data.attrs['_FillValue'])\n",
    "            if 'scale_factor' in lst_data.attrs:\n",
    "                lst_data = lst_data * lst_data.attrs['scale_factor']\n",
    "            lst_celsius = lst_data - 273.15\n",
    "            \n",
    "            lst_celsius.rio.write_crs(SINUSOIDAL_PROJ, inplace=True)\n",
    "            \n",
    "            tarih_match = re.search(r'\\.A(\\d{7})\\.', str(dosya_path))\n",
    "            if not tarih_match: continue\n",
    "            tarih = pd.to_datetime(tarih_match.group(1), format='%Y%j').strftime('%Y-%m-%d')\n",
    "            \n",
    "            iller_gdf_proj = iller_gdf.to_crs(lst_celsius.rio.crs)\n",
    "\n",
    "            for index, il in iller_gdf_proj.iterrows():\n",
    "                sehir_adi = il['sehir']\n",
    "                try:\n",
    "                    clipped = lst_celsius.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                    ortalama_sicaklik = float(clipped.mean())\n",
    "                    \n",
    "                    if pd.notna(ortalama_sicaklik):\n",
    "                        tum_sonuclar.append({\"sehir\": sehir_adi, \"tarih\": tarih, \"sicaklik_C\": ortalama_sicaklik})\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ HATA: '{dosya_path.name}' atlanıyor. -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_sonuclar:\n",
    "        print(\"\\n❌ Test işlemi başarısız. Hiçbir veri işlenemedi.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nTest verileri işlendi. CSV dosyası oluşturuluyor...\")\n",
    "    sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "    sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "    print(f\"✨ Test Başarılı! Sonuçlar '{CIKTI_CSV_DOSYASI}' dosyasına kaydedildi.\")\n",
    "    print(\"\\nCSV Dosyasının ilk 5 satırı:\")\n",
    "    print(sonuc_df.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    skorlari_hesapla_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f7108",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Yağış / Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00742a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneyi kuralım\n",
    "!pip install earthaccess -q\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- AYARLAR (YAĞIŞ VERİSİ İÇİN UYARLANDI) ---\n",
    "\n",
    "# Veri Seti Bilgileri\n",
    "VERI_SETI_KISA_ADI = 'GPM_3IMERGDF' # <-- GÜNCELLENDİ\n",
    "VERSIYON = '07'                # <-- GÜNCELLENDİ\n",
    "YIL = 2020                     # <-- İndirmek istediğin yılı buraya yaz\n",
    "\n",
    "# Türkiye'yi içine alan coğrafi kutu\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# Verilerin indirileceği klasör\n",
    "INDIRME_KLASORU = f\"NASA_Verileri/{VERI_SETI_KISA_ADI}/{YIL}\"\n",
    "\n",
    "# --- KOD BAŞLANGICI (DEĞİŞİKLİK YOK) ---\n",
    "\n",
    "print(\"NASA Earthdata sistemine giriş yapılıyor...\")\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    print(\"✅ Giriş başarılı.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Giriş başarısız. Lütfen .netrc dosyasını kontrol edin. Hata: {e}\")\n",
    "    exit()\n",
    "\n",
    "Path(INDIRME_KLASORU).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dosyalar '{INDIRME_KLASORU}' klasörüne indirilecek.\\n\")\n",
    "\n",
    "# Bitiş tarihini YIL'a göre doğru ayarlayan mantık\n",
    "baslangic_tarihi = f\"{YIL}-01-01\"\n",
    "bugun = datetime.now()\n",
    "\n",
    "if YIL < bugun.year:\n",
    "    bitis_tarihi = f\"{YIL}-12-31\"\n",
    "else:\n",
    "    bitis_tarihi = bugun.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Veri Seti: {VERI_SETI_KISA_ADI}, Yıl: {YIL}\")\n",
    "print(f\"Zaman Aralığı: {baslangic_tarihi} -> {bitis_tarihi}\")\n",
    "print(f\"Bölge: Türkiye ({TURKIYE_BOUNDING_BOX})\")\n",
    "print(\"Veriler aranıyor...\")\n",
    "\n",
    "try:\n",
    "    sonuclar = earthaccess.search_data(\n",
    "        short_name=VERI_SETI_KISA_ADI,\n",
    "        version=VERSIYON,\n",
    "        temporal=(baslangic_tarihi, bitis_tarihi),\n",
    "        bounding_box=TURKIYE_BOUNDING_BOX\n",
    "    )\n",
    "    \n",
    "    if sonuclar:\n",
    "        print(f\"✅ {len(sonuclar)} adet dosya bulundu. İndirme işlemi başlıyor...\")\n",
    "        print(\"❗ Bu işlem biraz zaman alabilir, lütfen sabırlı olun...\")\n",
    "        \n",
    "        earthaccess.download(sonuclar, local_path=INDIRME_KLASORU)\n",
    "        \n",
    "        print(f\"\\n✨ İndirme tamamlandı! {len(sonuclar)} dosya '{INDIRME_KLASORU}' klasörüne indirildi.\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠ Belirtilen tarih ve bölge için hiç dosya bulunamadı.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Arama veya indirme sırasında bir hata oluştu: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dfb84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rioxarray\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from shapely.geometry import box\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Test modunu aktive et ve işlenecek dosya sayısını belirle\n",
    "TEST_MODU = False\n",
    "TEST_DOSYA_SAYISI = 10\n",
    "\n",
    "YIL = 2020\n",
    "\n",
    "VERI_KLASORU = f\"NASA_Verileri/GPM_3IMERGDF/{YIL}\"\n",
    "GEOJSON_DOSYASI = \"tr-cities.json\"\n",
    "\n",
    "print(VERI_KLASORU)\n",
    "\n",
    "# Çıktı dosya adını test moduna göre ayarla\n",
    "if TEST_MODU:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_yagis_skorlari_{YIL}_TEST.csv\"\n",
    "else:\n",
    "    CIKTI_CSV_DOSYASI = f\"sehir_bazli_yagis_skorlari_{YIL}.csv\"\n",
    "\n",
    "YAGIS_DEGISKENI = 'precipitation'\n",
    "TURKIYE_BOUNDING_BOX = (25, 35, 45, 43)\n",
    "\n",
    "# --- TEK BİR DOSYAYI İŞLEYEN FONKSİYON ---\n",
    "def process_precipitation_file(dosya_path):\n",
    "    try:\n",
    "        ds = xr.open_dataset(dosya_path)\n",
    "        yagis_data = ds[YAGIS_DEGISKENI]\n",
    "\n",
    "        if 'time' in yagis_data.dims:\n",
    "            yagis_data = yagis_data.squeeze('time', drop=True)\n",
    "\n",
    "        yagis_data.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "        yagis_data.rio.write_crs(\"epsg:4236\", inplace=True)\n",
    "        \n",
    "        min_lon, min_lat, max_lon, max_lat = TURKIYE_BOUNDING_BOX\n",
    "        turkey_geom = box(min_lon, min_lat, max_lon, max_lat)\n",
    "        yagis_turkiye = yagis_data.rio.clip([turkey_geom], \"epsg:4236\")\n",
    "        \n",
    "        tarih_match = re.search(r'\\.(\\d{8})-S', str(dosya_path))\n",
    "        if not tarih_match: return []\n",
    "        tarih = pd.to_datetime(tarih_match.group(1), format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "        \n",
    "        iller_gdf = gpd.read_file(GEOJSON_DOSYASI)\n",
    "        iller_gdf = iller_gdf[['name', 'geometry']].rename(columns={'name': 'sehir'})\n",
    "        iller_gdf_proj = iller_gdf.to_crs(yagis_data.rio.crs)\n",
    "        \n",
    "        gunluk_sonuclar = []\n",
    "        for index, il in iller_gdf_proj.iterrows():\n",
    "            sehir_adi = il['sehir']\n",
    "            try:\n",
    "                clipped = yagis_turkiye.rio.clip([il['geometry']], drop=True, all_touched=True)\n",
    "                ortalama_yagis = float(clipped.mean())\n",
    "                \n",
    "                if pd.notna(ortalama_yagis):\n",
    "                    gunluk_sonuclar.append({\n",
    "                        \"sehir\": sehir_adi,\n",
    "                        \"tarih\": tarih,\n",
    "                        \"yagis_mm_gun\": ortalama_yagis\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "        return gunluk_sonuclar\n",
    "    except Exception as e:\n",
    "        # print(f\"HATA: {dosya_path.name} işlenemedi. Sebep: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- ANA KOD ---\n",
    "if __name__ == '__main__':\n",
    "    veri_yolu = Path(VERI_KLASORU)\n",
    "    nc4_dosyalari = sorted(list(veri_yolu.glob('*.nc4')))\n",
    "    \n",
    "    # Test modu aktifse, dosya listesini kısalt\n",
    "    if TEST_MODU:\n",
    "        nc4_dosyalari = nc4_dosyalari[:TEST_DOSYA_SAYISI]\n",
    "        print(f\"🐞 TEST MODU AKTİF: Sadece ilk {len(nc4_dosyalari)} dosya işlenecek.\")\n",
    "\n",
    "    if not nc4_dosyalari:\n",
    "        print(f\"❌ HATA: '{veri_yolu}' klasöründe .nc4 dosyası bulunamadı.\")\n",
    "    else:\n",
    "        print(f\"İşlenmek üzere {len(nc4_dosyalari)} adet yağış dosyası seçildi.\")\n",
    "        print(\"Optimize edilmiş paralel işleme başlıyor...\")\n",
    "\n",
    "        tum_sonuclar = []\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results_iterator = list(tqdm(executor.map(process_precipitation_file, nc4_dosyalari), total=len(nc4_dosyalari)))\n",
    "        \n",
    "        for result in results_iterator:\n",
    "            tum_sonuclar.extend(result)\n",
    "\n",
    "        if not tum_sonuclar:\n",
    "            print(\"\\n❌ İşlem bitti ancak hiçbir veri işlenemedi.\")\n",
    "        else:\n",
    "            print(\"\\nTest verileri işlendi. CSV dosyası oluşturuluyor...\")\n",
    "            sonuc_df = pd.DataFrame(tum_sonuclar)\n",
    "            sonuc_df.to_csv(CIKTI_CSV_DOSYASI, index=False, encoding='utf-8')\n",
    "            print(f\"✨ Test Başarılı! Sonuçlar '{CIKTI_CSV_DOSYASI}' dosyasına kaydedildi.\")\n",
    "            print(\"\\nCSV Dosyasının ilk 5 satırı:\")\n",
    "            print(sonuc_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c003285",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model Eğitimi / Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ed185",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas sklearn prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66413457",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prophet import Prophet\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os # Dosya yollarını birleştirmek için eklendi\n",
    "\n",
    "# Prophet ve Stan'den gelen gereksiz log mesajlarını kapatalım\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# DEĞİŞİKLİK: Artık birden fazla yılı işliyoruz\n",
    "YILLAR = [2020, 2021, 2022, 2023, 2024,2025] \n",
    "ANA_KLASOR = \"/content/drive/MyDrive/Nasa/Tum_veri/\" # Ana dosya yolu\n",
    "\n",
    "# Tahmin edilecek özel tarihler\n",
    "TAHMIN_TARIHLERI = ['2027-01-15', '2027-04-15', '2027-07-15', '2027-10-15']\n",
    "\n",
    "# Çıktı dosyalarının adları\n",
    "YIL_ARALIGI_STR = f\"{YILLAR[0]}-{YILLAR[-1]}\"\n",
    "FINAL_DAILY_CSV = f\"EcoPulse_Skorlari_GUNLUK_{YIL_ARALIGI_STR}_Temizlenmis.csv\"\n",
    "FINAL_MONTHLY_CSV = f\"EcoPulse_Skorlari_AYLIK_{YIL_ARALIGI_STR}_Temizlenmis.csv\" # YENİ ÇIKTI\n",
    "FINAL_FORECAST_CSV = f\"EcoPulse_2027_Tahminleri_{YIL_ARALIGI_STR}_Verisiyle.csv\"\n",
    "\n",
    "# --- 1. Adım: Tüm Yıllar İçin Veri Hazırlama ve TEMİZLEME ---\n",
    "def cok_yilli_veri_hazirla():\n",
    "    print(f\"{YIL_ARALIGI_STR} yılları arasındaki tüm veriler birleştiriliyor...\")\n",
    "    \n",
    "    tum_datalar = []\n",
    "    for yil in YILLAR:\n",
    "        try:\n",
    "            # Her yıl için dosya yollarını dinamik olarak oluştur\n",
    "            no2_csv = os.path.join(ANA_KLASOR, f\"hava_kirliligi/sehir_bazli_no2_skorlari_{yil}.csv\")\n",
    "            gece_temp_csv = os.path.join(ANA_KLASOR, f\"sıcaklık_gece/sehir_bazli_sicaklik_skorlari_{yil}.csv\")\n",
    "            gunduz_temp_csv = os.path.join(ANA_KLASOR, f\"sıcaklık_gunduz/sehir_bazli_sicaklik_skorlari_{yil}_gunduz.csv\")\n",
    "            yagis_csv = os.path.join(ANA_KLASOR, f\"yagıs/sehir_bazli_yagis_skorlari_{yil}.csv\")\n",
    "\n",
    "            df_no2 = pd.read_csv(no2_csv)\n",
    "            df_gece_temp = pd.read_csv(gece_temp_csv)\n",
    "            df_gunduz_temp = pd.read_csv(gunduz_temp_csv)\n",
    "            df_yagis = pd.read_csv(yagis_csv)\n",
    "            \n",
    "            df_gece_temp = df_gece_temp.rename(columns={'sicaklik_C': 'sicaklik_gece_C'})\n",
    "            df_gunduz_temp = df_gunduz_temp.rename(columns={'sicaklik_C': 'sicaklik_gunduz_C'})\n",
    "            \n",
    "            df_merged = pd.merge(df_no2, df_gece_temp, on=['sehir', 'tarih'])\n",
    "            df_merged = pd.merge(df_merged, df_gunduz_temp, on=['sehir', 'tarih'])\n",
    "            df_merged = pd.merge(df_merged, df_yagis, on=['sehir', 'tarih'])\n",
    "            \n",
    "            tum_datalar.append(df_merged)\n",
    "            print(f\"✅ {yil} yılı verileri başarıyla okundu.\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"❌ UYARI: {yil} yılı için bir veya daha fazla dosya bulunamadı. Bu yıl atlanıyor. Hata: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not tum_datalar:\n",
    "        print(\"❌ HATA: Hiçbir yıla ait veri okunamadı. Lütfen dosya yollarını ve adlarını kontrol edin.\")\n",
    "        return None\n",
    "\n",
    "    # Tüm yılların verilerini tek bir dataframe'de birleştir\n",
    "    df_final = pd.concat(tum_datalar, ignore_index=True)\n",
    "    df_final['tarih'] = pd.to_datetime(df_final['tarih'])\n",
    "    \n",
    "    # --- VERİ TEMİZLEME ADIMI ---\n",
    "    print(\"\\nVeri temizleme adımı başlatılıyor...\")\n",
    "    orijinal_satir_sayisi = len(df_final)\n",
    "    MIN_SICAKLIK, MAX_SICAKLIK = -25.0, 55.0\n",
    "    \n",
    "    df_final = df_final[\n",
    "        (df_final['sicaklik_gece_C'].between(MIN_SICAKLIK, MAX_SICAKLIK)) &\n",
    "        (df_final['sicaklik_gunduz_C'].between(MIN_SICAKLIK, MAX_SICAKLIK))\n",
    "    ]\n",
    "    \n",
    "    kaldirilan_satir_sayisi = orijinal_satir_sayisi - len(df_final)\n",
    "    print(f\"✅ Veri temizleme tamamlandı. {kaldirilan_satir_sayisi} adet hatalı satır kaldırıldı.\")\n",
    "    \n",
    "    # --- GÜNLÜK ECOPULSE HESAPLAMA ---\n",
    "    print(\"\\nGünlük EcoPulse skorları hesaplanıyor...\")\n",
    "    df_final['sicaklik_farki_DTR'] = df_final['sicaklik_gunduz_C'] - df_final['sicaklik_gece_C']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    df_final['P_norm'] = (1 - scaler.fit_transform(df_final[['no2_skoru']])) * 100\n",
    "    df_final['H_norm'] = (1 - scaler.fit_transform(df_final[['sicaklik_gece_C']])) * 100\n",
    "    df_final['R_norm'] = scaler.fit_transform(df_final[['yagis_mm_gun']]) * 100\n",
    "    df_final['DTR_norm'] = (1 - scaler.fit_transform(df_final[['sicaklik_farki_DTR']])) * 100\n",
    "    \n",
    "    df_final['EcoPulse'] = (0.30 * df_final['P_norm']) + (0.30 * df_final['DTR_norm']) + \\\n",
    "                           (0.20 * df_final['R_norm']) + (0.20 * df_final['H_norm'])\n",
    "    print(\"✅ Günlük EcoPulse skorları hazır.\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# --- Model Fonksiyonu (Değişiklik yok) ---\n",
    "def train_and_forecast_city_specific_dates(args):\n",
    "    # ... (Bu fonksiyon önceki kod ile aynı, değişiklik yok)\n",
    "    sehir_adi, df_sehir, dates_to_predict = args\n",
    "    try:\n",
    "        if len(df_sehir) < 2: return []\n",
    "        df_prophet = df_sehir[['tarih', 'EcoPulse']].rename(columns={'tarih': 'ds', 'EcoPulse': 'y'})\n",
    "        df_prophet['cap'], df_prophet['floor'] = 100, 0\n",
    "        model = Prophet(growth='logistic', yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "        model.fit(df_prophet)\n",
    "        future = pd.DataFrame({'ds': dates_to_predict}); future['cap'], future['floor'] = 100, 0\n",
    "        forecast = model.predict(future)\n",
    "        return [{\"sehir\": sehir_adi, \"tahmin_tarihi\": row['ds'].strftime('%Y-%m-%d'), \"tahmini_ecopulse_skoru\": row['yhat']} for _, row in forecast.iterrows()]\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: {sehir_adi} şehri için tahmin yapılırken bir sorun oluştu: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- ANA KOD ---\n",
    "if __name__ == '__main__':\n",
    "    df_gunluk_ecopulse = cok_yilli_veri_hazirla()\n",
    "    \n",
    "    if df_gunluk_ecopulse is not None and not df_gunluk_ecopulse.empty:\n",
    "        # --- GÜNLÜK VERİYİ KAYDET ---\n",
    "        print(f\"\\nGünlük hesaplanan EcoPulse skorları '{FINAL_DAILY_CSV}' dosyasına kaydediliyor...\")\n",
    "        kolonlar = ['sehir', 'tarih', 'EcoPulse', 'no2_skoru', 'sicaklik_gece_C', 'sicaklik_gunduz_C', 'yagis_mm_gun']\n",
    "        df_gunluk_ecopulse[kolonlar].to_csv(FINAL_DAILY_CSV, index=False, encoding='utf-8', float_format='%.2f')\n",
    "        print(f\"✅ Günlük veriler başarıyla kaydedildi.\")\n",
    "\n",
    "        # --- YENİ ADIM: AYLIK ORTALAMALARI HESAPLA VE KAYDET ---\n",
    "        print(f\"\\nAylık ortalama EcoPulse skorları '{FINAL_MONTHLY_CSV}' dosyasına kaydediliyor...\")\n",
    "        df_aylik = df_gunluk_ecopulse.groupby(['sehir', pd.Grouper(key='tarih', freq='M')])['EcoPulse'].mean().reset_index()\n",
    "        df_aylik = df_aylik.rename(columns={'EcoPulse': 'aylik_ortalama_ecopulse'})\n",
    "        df_aylik['tarih'] = df_aylik['tarih'].dt.to_period('M')\n",
    "        df_aylik['aylik_ortalama_ecopulse'] = df_aylik['aylik_ortalama_ecopulse'].round(2)\n",
    "        df_aylik.to_csv(FINAL_MONTHLY_CSV, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Aylık ortalama verileri başarıyla kaydedildi.\")\n",
    "        print(\"Aylık verilerden örnek:\")\n",
    "        print(df_aylik.head())\n",
    "        # --- AYLIK ORTALAMA ADIMI BİTTİ ---\n",
    "\n",
    "        # --- MODEL EĞİTİMİ VE TAHMİN ---\n",
    "        predict_dates = pd.to_datetime(TAHMIN_TARIHLERI)\n",
    "        sehirler = df_gunluk_ecopulse['sehir'].unique()\n",
    "        sehir_datalari = [(sehir, df_gunluk_ecopulse[df_gunluk_ecopulse['sehir'] == sehir], predict_dates) for sehir in sehirler]\n",
    "        \n",
    "        print(f\"\\n{len(sehirler)} şehir için paralel olarak {len(predict_dates)} özel gün tahmini yapılıyor...\")\n",
    "        \n",
    "        tum_tahminler = []\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(train_and_forecast_city_specific_dates, sehir_datalari), total=len(sehir_datalari)))\n",
    "\n",
    "        for res in results: tum_tahminler.extend(res)\n",
    "        \n",
    "        if tum_tahminler:\n",
    "            print(\"\\n✅ Tüm şehirler için özel gün tahminleri tamamlandı.\")\n",
    "            df_tahmin = pd.DataFrame(tum_tahminler)\n",
    "            df_tahmin['tahmini_ecopulse_skoru'] = df_tahmin['tahmini_ecopulse_skoru'].clip(0, 100).round(2)\n",
    "            df_tahmin.to_csv(FINAL_FORECAST_CSV, index=False, encoding='utf-8')\n",
    "            print(f\"✨ Tahmin sonuçları '{FINAL_FORECAST_CSV}' dosyasına kaydedildi.\")\n",
    "        else:\n",
    "            print(\"\\n❌ Tahmin üretilemedi.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Hiç geçerli veri kalmadığı için hiçbir işlem yapılamadı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52c7c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- AYARLAR ---\n",
    "# Lütfen ana kodun ürettiği AYLIK ortalama dosyasının adının\n",
    "# bu olduğundan emin ol. Yıl aralığı farklıysa (örn: 2020-2024),\n",
    "# bu değişkeni güncelleyebilirsin.\n",
    "YIL_ARALIGI_STR = \"2020-2025\" \n",
    "AYLIK_ORTALAMA_DOSYASI = f\"EcoPulse_Skorlari_AYLIK_{YIL_ARALIGI_STR}_Temizlenmis.csv\"\n",
    "\n",
    "def kategori_sinirlarini_bul():\n",
    "    \"\"\"\n",
    "    Aylık EcoPulse skorlarını analiz eder ve web sitesinde kullanılacak\n",
    "    Kırmızı, Sarı, Yeşil renk kategorileri için en uygun sınırları önerir.\n",
    "    \"\"\"\n",
    "    # Dosyanın var olup olmadığını kontrol et\n",
    "    if not os.path.exists(AYLIK_ORTALAMA_DOSYASI):\n",
    "        print(f\"❌ HATA: '{AYLIK_ORTALAMA_DOSYASI}' dosyası bulunamadı.\")\n",
    "        print(\"Lütfen dosya adının doğru olduğundan ve bu kodun ilgili dosyayla aynı klasörde olduğundan emin ol.\")\n",
    "        return\n",
    "\n",
    "    print(f\"'{AYLIK_ORTALAMA_DOSYASI}' dosyası okunuyor ve analiz ediliyor...\")\n",
    "    \n",
    "    # 1. Adım: Veriyi oku\n",
    "    df_aylik = pd.read_csv(AYLIK_ORTALAMA_DOSYASI)\n",
    "    skor_kolonu = 'aylik_ortalama_ecopulse'\n",
    "\n",
    "    # 2. Adım: Verinin genel istatistiksel dağılımını göster\n",
    "    # Bu, verinin genel yapısını anlamamızı sağlar (min, max, ortalama vb.)\n",
    "    print(\"\\n--- Aylık EcoPulse Skorlarının Genel Dağılımı ---\")\n",
    "    distribution_stats = df_aylik[skor_kolonu].describe()\n",
    "    print(distribution_stats)\n",
    "    \n",
    "    # 3. Adım: Yüzdelik dilim (percentile) yöntemini kullanarak sınırları hesapla\n",
    "    # Bu yöntem, veri setini 3 eşit parçaya böler.\n",
    "    # Her renk kategorisinin veri setinde anlamlı sayıda örnek içermesini sağlar.\n",
    "    try:\n",
    "        sinir_kirmizi_sari = np.percentile(df_aylik[skor_kolonu], 33)\n",
    "        sinir_sari_yesil = np.percentile(df_aylik[skor_kolonu], 66)\n",
    "        \n",
    "        min_skor = distribution_stats['min']\n",
    "        max_skor = distribution_stats['max']\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 WEB SİTESİ İÇİN RENK KATEGORİSİ ÖNERİLERİ 📊\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nSenin ilk önerin:\")\n",
    "        print(f\"🔴 Kırmızı: 0 - 35\")\n",
    "        print(f\"🟡 Sarı   : 35 - 65\")\n",
    "        print(f\"🟢 Yeşil  : 65 - 100\")\n",
    "        \n",
    "        # Sonuçları daha temiz ve kullanılabilir hale getirelim (tam sayılara yuvarlayarak)\n",
    "        alt_sinir_sari = int(round(sinir_kirmizi_sari))\n",
    "        alt_sinir_yesil = int(round(sinir_sari_yesil))\n",
    "        \n",
    "        print(f\"\\n📈 Veri Dağılımına Göre Önerilen Yeni Aralıklar:\")\n",
    "        print(f\"🔴 Kırmızı (Düşük Performans): 0 - {alt_sinir_sari}\")\n",
    "        print(f\"🟡 Sarı   (Orta Performans)  : {alt_sinir_sari} - {alt_sinir_yesil}\")\n",
    "        print(f\"🟢 Yeşil  (İyi Performans)   : {alt_sinir_yesil} - 100\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nBu aralıklar, veri setindeki skorları yaklaşık 3 eşit gruba ayırır.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nHesaplama sırasında bir hata oluştu: {e}\")\n",
    "\n",
    "# Ana fonksiyonu çalıştır\n",
    "if __name__ == '__main__':\n",
    "    kategori_sinirlarini_bul()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
